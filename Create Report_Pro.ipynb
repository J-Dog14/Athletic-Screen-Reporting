{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:26:40.385699Z",
     "start_time": "2025-06-19T18:26:40.296942Z"
    }
   },
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the folder containing your txt files\n",
    "folder_path = 'D:/Athletic Screen 2.0/Output Files/'\n",
    "db_path = 'D:/Athletic Screen 2.0/Output Files/movement_database_v2.db'\n",
    "\n",
    "# Delete the database file if it exists to start fresh\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(f\"Deleted existing database at {db_path}\")\n",
    "\n",
    "# Connect to the SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the corrected table schemas for each movement\n",
    "table_schemas = {\n",
    "    'CMJ': '''CREATE TABLE IF NOT EXISTS CMJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "\n",
    "    'PPU': '''CREATE TABLE IF NOT EXISTS PPU (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT,\n",
    "        date TEXT,\n",
    "        trial_name TEXT,\n",
    "        JH_IN REAL,\n",
    "        Peak_Power REAL,\n",
    "        PP_FORCEPLATE REAL,\n",
    "        Force_at_PP REAL,\n",
    "        Vel_at_PP REAL,\n",
    "        PP_W_per_kg REAL\n",
    "    )''',\n",
    "    \n",
    "    'DJ':  '''CREATE TABLE IF NOT EXISTS DJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL,\n",
    "                CT REAL,\n",
    "                RSI REAL\n",
    "              )''',\n",
    "\n",
    "    'SLV': '''CREATE TABLE IF NOT EXISTS SLV (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT, \n",
    "                trial_name TEXT,\n",
    "                side TEXT,\n",
    "                JH_IN REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "    'NMT': '''CREATE TABLE IF NOT EXISTS NMT (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT, \n",
    "                trial_name TEXT,\n",
    "                NUM_TAPS_10s REAL,\n",
    "                NUM_TAPS_20s REAL,\n",
    "                NUM_TAPS_30s REAL,\n",
    "                NUM_TAPS REAL\n",
    "              )'''\n",
    "}\n",
    "\n",
    "# Create the tables in the database (if they don't exist)\n",
    "for schema in table_schemas.values():\n",
    "    cursor.execute(schema)\n",
    "\n",
    "# Function to extract the client's name from the first line of the file\n",
    "def extract_name(line):\n",
    "    match = re.search(r'Data\\\\(.*?)[_\\\\]', line)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_date(line):\n",
    "    \"\"\"\n",
    "    Looks for a segment like 2025-05-22_ in the first-line path returned by Cortex.\n",
    "    Returns '2025-05-22' or None if not found.\n",
    "    \"\"\"\n",
    "    m = re.search(r'\\\\(\\d{4}-\\d{2}-\\d{2})_', line)\n",
    "    return m.group(1) if m else None\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Function to insert data into the appropriate table\n",
    "def insert_data_into_table(table_name, name, trial_name, variables):\n",
    "    \"\"\"\n",
    "    `variables` still contains the leading dummy “1”.\n",
    "    After we drop that each txt gives:\n",
    "        CMJ : 8 numbers\n",
    "        DJ  : 8 numbers\n",
    "        SLV : 6 numbers\n",
    "        NMT : 4 numbers\n",
    "    We pick only the columns we store.\n",
    "    \"\"\"\n",
    "    v = variables[1:]          # drop the leading “1”\n",
    "\n",
    "    if table_name == 'CMJ':\n",
    "        # keep indices 0,1,4,5,6,7  (→ six values)\n",
    "        vals = [v[i] for i in (0, 1, 4, 5, 6, 7)]\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO CMJ\n",
    "               (name, date, trial_name,\n",
    "                JH_IN, Peak_Power,\n",
    "                PP_FORCEPLATE, Force_at_PP, Vel_at_PP,\n",
    "                PP_W_per_kg)\n",
    "               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "            (name, date, trial_name, *vals)\n",
    "        )\n",
    "    \n",
    "    elif table_name == 'PPU':\n",
    "        # mirror CMJ selection/order\n",
    "        vals = [v[i] for i in (0, 1, 4, 5, 6, 7)]\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO PPU\n",
    "               (name, date, trial_name,\n",
    "                JH_IN, Peak_Power,\n",
    "                PP_FORCEPLATE, Force_at_PP, Vel_at_PP,\n",
    "                PP_W_per_kg)\n",
    "               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "            (name, date, trial_name, *vals)\n",
    "        )\n",
    "\n",
    "    elif table_name == 'DJ':\n",
    "        # keep every value (8 numbers)\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO DJ\n",
    "               (name, date, trial_name,\n",
    "                JH_IN, Peak_Power,\n",
    "                PP_FORCEPLATE, Force_at_PP, Vel_at_PP,\n",
    "                CT, RSI, PP_W_per_kg)\n",
    "               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "            (name, date, trial_name, *v)        # 8 numbers + 2 strings = 10\n",
    "        )\n",
    "\n",
    "    elif table_name == 'SLV':\n",
    "        side = 'Left' if 'SLVL' in trial_name else 'Right'\n",
    "        # keep indices 0,2,3,4,5  (→ five values)\n",
    "        vals = [v[i] for i in (0, 2, 3, 4, 5)]\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO SLV\n",
    "               (name, date, trial_name, side,\n",
    "                JH_IN,\n",
    "                PP_FORCEPLATE, Force_at_PP, Vel_at_PP,\n",
    "                PP_W_per_kg)\n",
    "               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "            (name, date, trial_name, side, *vals)\n",
    "        )\n",
    "\n",
    "    elif table_name == 'NMT':\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO NMT\n",
    "               (name, date, trial_name,\n",
    "                NUM_TAPS_10s, NUM_TAPS_20s, NUM_TAPS_30s, NUM_TAPS)\n",
    "               VALUES (?, ?, ?, ?, ?, ?, ?)\"\"\",\n",
    "            (name, date, trial_name, *v)\n",
    "        )\n",
    "# Loop through the txt files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        trial_name = os.path.splitext(file_name)[0]\n",
    "        \n",
    "        # Determine which table the file belongs to\n",
    "        if 'CMJ' in trial_name:\n",
    "            table_name = 'CMJ'\n",
    "        elif 'PPU' in trial_name:\n",
    "            table_name = 'PPU'\n",
    "        elif 'DJ' in trial_name:\n",
    "            table_name = 'DJ'\n",
    "        elif 'SLVL' in trial_name or 'SLVR' in trial_name:\n",
    "            table_name = 'SLV'\n",
    "        elif 'NMT' in trial_name:\n",
    "            table_name = 'NMT'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Load the data from the txt file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                # Extract the name from the first line\n",
    "                first_line = f.readline().strip()\n",
    "                name = extract_name(first_line)\n",
    "                date = extract_date(first_line)\n",
    "\n",
    "                # Print the extracted name to verify\n",
    "                print(f\"File: {file_name}, Extracted Name: {name}\")\n",
    "\n",
    "                if not name:\n",
    "                    print(f\"Name extraction failed for {file_name}, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Read all lines until we find the line with the actual numeric data\n",
    "                # --- replace the old for-loop (line_num, line) with this: -------------------\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                \n",
    "                    if re.match(r'^[-+]?\\d', line):          # first real numeric row\n",
    "                        variables = [float(v) for v in line.split()]\n",
    "                        print(f\"Processing file: {file_name}, Variables: {variables}\")\n",
    "                        insert_data_into_table(table_name, name, trial_name, variables)\n",
    "                        break\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with file {file_name}: {e}\")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data successfully inserted into the database.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing database at D:/Athletic Screen 2.0/Output Files/movement_database_v2.db\n",
      "File: CMJ1.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: CMJ1.txt, Variables: [1.0, 15.5, 516.0, 2.27, 228.0, 1196.5, 2318.52, 516.07, 14.96]\n",
      "File: SLVL1.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVL1.txt, Variables: [1.0, 9.6, 6216.0, 944.8, 2044.4, 462.1, 11.81]\n",
      "File: CMJ2.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: CMJ2.txt, Variables: [1.0, 16.6, 523.0, 2.38, 219.0, 1207.8, 2307.87, 523.32, 15.1]\n",
      "File: CMJ3.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: CMJ3.txt, Variables: [1.0, 15.5, 516.0, 2.27, 228.0, 1196.5, 2318.52, 516.07, 14.96]\n",
      "File: DJ1.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: DJ1.txt, Variables: [1.0, 19.8, 2187.0, 1894.7, 2686.37, 705.3, 0.63, 1.6, 23.68]\n",
      "File: DJ2.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: DJ2.txt, Variables: [1.0, 18.2, 1750.0, 1871.3, 2507.22, 746.35, 0.67, 1.38, 23.39]\n",
      "File: DJ3.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: DJ3.txt, Variables: [1.0, 18.6, 2012.0, 1586.0, 2494.26, 635.88, 0.6, 1.58, 19.83]\n",
      "File: SLVL2.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVL2.txt, Variables: [1.0, 10.2, 6308.0, 1028.0, 2075.3, 495.3, 12.85]\n",
      "File: SLVL3.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVL3.txt, Variables: [1.0, 9.8, 6239.0, 979.9, 1985.6, 493.5, 12.25]\n",
      "File: SLVR1.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVR1.txt, Variables: [1.0, 8.4, 6020.0, 965.8, 1992.0, 484.8, 12.07]\n",
      "File: SLVR2.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVR2.txt, Variables: [1.0, 9.8, 6239.0, 1024.6, 2111.4, 485.3, 12.81]\n",
      "File: NMT.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: NMT.txt, Variables: [1.0, 75.0, 78.0, 62.0, 211.0]\n",
      "File: SLVR3.txt, Extracted Name: Zach, Vennaro\n",
      "Processing file: SLVR3.txt, Variables: [1.0, 10.5, 6355.0, 991.9, 2078.8, 477.1, 12.4]\n",
      "Data successfully inserted into the database.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases opened successfully.\n",
      "Client Name: Jalen Hollins\n",
      "Document saved at: G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Athletic_Report_Jalen_Hollins.docx\n",
      "Images saved at G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Images\\final_page.png\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "# Creates full report for age group comparison\n",
    "\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from datetime import date\n",
    "import tempfile\n",
    "import docx2txt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os, re\n",
    "from scipy import integrate, stats\n",
    "import glob as globmod\n",
    "\n",
    "# -------- style to match your dark report --------\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"#181818\",\n",
    "    \"axes.facecolor\"  : \"#303030\",\n",
    "    \"axes.edgecolor\"  : \"white\",\n",
    "    \"axes.labelcolor\" : \"slategrey\",\n",
    "    \"xtick.color\"     : \"lightgrey\",\n",
    "    \"ytick.color\"     : \"lightgrey\",\n",
    "    \"grid.color\"      : \"dimgrey\",\n",
    "    \"text.color\"      : \"white\",\n",
    "})\n",
    "# Corrected file paths with raw strings to handle backslashes properly\n",
    "client_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\movement_database_v2.db'\n",
    "reference_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\Athletic_Screen_Pro_data_v2.db'\n",
    "\n",
    "# Ensure the paths are valid and accessible\n",
    "if not os.path.exists(client_db_path):\n",
    "    print(f\"Client database not found at {client_db_path}\")\n",
    "if not os.path.exists(reference_db_path):\n",
    "    print(f\"Reference database not found at {reference_db_path}\")\n",
    "\n",
    "# Connect to the client and reference databases\n",
    "client_conn = sqlite3.connect(client_db_path)\n",
    "reference_conn = sqlite3.connect(reference_db_path)\n",
    "client_cursor = client_conn.cursor()\n",
    "reference_cursor = reference_conn.cursor()\n",
    "\n",
    "print(\"Databases opened successfully.\")\n",
    "\n",
    "# Fetch the client's name from the database (assuming the 'name' column is in all tables)\n",
    "client_cursor.execute(\"SELECT DISTINCT name FROM CMJ\")  # Change table if necessary\n",
    "client_name = client_cursor.fetchone()[0]  # Get the first row and first column\n",
    "print(f\"Client Name: {client_name}\")\n",
    "\n",
    "# ---------- build unique export paths (date-stamped, no overwrite) -----\n",
    "client_cursor.execute(\"SELECT MAX(date) FROM CMJ WHERE name = ?\", (client_name,))\n",
    "assessment_date = client_cursor.fetchone()[0]          # e.g. '2025-05-22'\n",
    "if not assessment_date:                                # fallback to today\n",
    "    assessment_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "reports_dir = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports'\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "parts = client_name.split(', ')\n",
    "client_name_rev = f\"{parts[1]}_{parts[0]}\" if len(parts) == 2 else client_name\n",
    "base_name = f\"Athletic_Report_{client_name_rev}_{assessment_date}\"\n",
    "\n",
    "output_filename = os.path.join(reports_dir, base_name + \".docx\")\n",
    "img_output_directory = os.path.join(reports_dir, \"Images\", base_name)\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# auto-increment if the same file already exists\n",
    "counter = 1\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = os.path.join(\n",
    "        reports_dir, f\"{base_name}_{counter}.docx\"\n",
    "    )\n",
    "    img_output_directory = os.path.join(\n",
    "        reports_dir, \"Images\", f\"{base_name}_{counter}\"\n",
    "    )\n",
    "    os.makedirs(img_output_directory, exist_ok=True)\n",
    "    counter += 1\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Helper function to calculate percentile\n",
    "def calculate_percentile(value, reference_data):\n",
    "    return stats.percentileofscore(reference_data, value)\n",
    "\n",
    "# ─── UPDATED generate_bar_graph ────────────────────────────────────────────────\n",
    "def generate_bar_graph(variable, client_value, reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ red      = client MAX (best trial **within the same movement table**)\n",
    "    ─ violet   = client MEAN (average of those trials)\n",
    "\n",
    "    • No code outside this function needs to change.\n",
    "    • If the caller still passes one score, that is fine; this function looks\n",
    "      up any matching trials on the same assessment day and combines them.\n",
    "    • RSI histograms use 0.25-wide bins so you see bars at 1.25, 1.50, 1.75 …\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, sqlite3, re\n",
    "\n",
    "    # ───────────── lookup: which movement table are we dealing with? ─────────\n",
    "    column_table_map = {\"CMJ\": [\"CMJ\"], \"PPU\": [\"PPU\"], \"DJ\": [\"DJ\"], \"SLV\": [\"SLV\"], \"NMT\": [\"NMT\"]}\n",
    "    table_guess = None\n",
    "    for tbl in column_table_map:       \n",
    "        try:\n",
    "            reference_cursor.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "            if reference_cursor.fetchone()[0] == len(reference_data):\n",
    "                table_guess = tbl\n",
    "                break\n",
    "        except sqlite3.OperationalError:\n",
    "            continue\n",
    "    if table_guess is None:               # fallback\n",
    "        for tbl in column_table_map:\n",
    "            try:\n",
    "                client_cursor.execute(f\"SELECT 1 FROM {tbl} LIMIT 1\")\n",
    "                table_guess = tbl\n",
    "                break\n",
    "            except sqlite3.OperationalError:\n",
    "                continue\n",
    "\n",
    "    # ───────────── gather all trials for this athlete / table / day ──────────\n",
    "    scores = []\n",
    "    if table_guess:\n",
    "        # Some tables (e.g., NMT) may not have trial_name. If so, fall back safely.\n",
    "        try:\n",
    "            client_cursor.execute(\n",
    "                f\"SELECT trial_name FROM {table_guess} WHERE name=? LIMIT 1\",\n",
    "                (client_name,),\n",
    "            )\n",
    "            row = client_cursor.fetchone()\n",
    "            date_prefix = None\n",
    "            if row and row[0]:\n",
    "                m = re.match(r\"(\\d{4}[-_]\\d{2}[-_]\\d{2})\", row[0])\n",
    "                date_prefix = m.group(1) if m else None\n",
    "    \n",
    "            if date_prefix:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=? AND trial_name LIKE ?\"\n",
    "                client_cursor.execute(q, (client_name, f\"{date_prefix}%\"))\n",
    "            else:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=?\"\n",
    "                client_cursor.execute(q, (client_name,))\n",
    "    \n",
    "            scores = [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "        except sqlite3.OperationalError:\n",
    "            # No trial_name column (likely NMT) → just use the single value\n",
    "            scores = [client_value]\n",
    "    \n",
    "    # fallback if still empty\n",
    "    if not scores:\n",
    "        scores = [client_value]\n",
    "\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    c_mean = scores.mean()\n",
    "    \n",
    "    # --- choose the extreme we draw as the red line ---------------------------\n",
    "    if variable.upper() == \"CT\":        # Contact-Time → use the LOWEST value\n",
    "        c_extreme = scores.min()\n",
    "        extreme_label = \"Client Min\"\n",
    "    else:                               # every other metric → highest value\n",
    "        c_extreme = scores.max()\n",
    "        extreme_label = \"Client Max\"\n",
    "\n",
    "    perc_mean = calculate_percentile(c_mean, reference_data)\n",
    "\n",
    "    # ──────────────────────────── plotting begins ────────────────────────────\n",
    "    plt.figure(facecolor=\"#181818\")\n",
    "    ax = plt.subplot(111, facecolor=\"#303030\")\n",
    "\n",
    "    reference_plotted = False  # flag to avoid double-plotting\n",
    "\n",
    "    # ---------- RSI special case: 0.25-wide bars & custom ticks -------------\n",
    "    if variable.upper() == \"RSI\":\n",
    "        lo = np.floor(reference_data.min() / 0.25) * 0.25\n",
    "        hi = np.ceil(reference_data.max() / 0.25) * 0.25\n",
    "        bins = np.arange(lo, hi + 0.25, 0.25)      # bin edges\n",
    "        centers = bins[:-1]                         # bar positions\n",
    "\n",
    "        counts, _ = np.histogram(reference_data, bins=bins)\n",
    "        ax.bar(\n",
    "            centers,\n",
    "            counts,\n",
    "            width=0.25,\n",
    "            align=\"edge\",\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "        # after counts, _ = np.histogram(...)\n",
    "        for x, h in zip(centers, counts):\n",
    "            if h == 0:                        # empty bin → draw thin outline\n",
    "                ax.bar(x, 1e-6, width=.25, align='edge',\n",
    "                       color='none', edgecolor='#404040', linewidth=.5)\n",
    "\n",
    "        ax.set_xticks(centers)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\" for x in centers], color=\"lightgrey\")\n",
    "\n",
    "        reference_plotted = True  # we already drew the reference bars\n",
    "    else:\n",
    "        bins = 20  # default bin count\n",
    "\n",
    "    # ---------- draw reference histogram when not plotted above -------------\n",
    "    if not reference_plotted:\n",
    "        ax.hist(\n",
    "            reference_data,\n",
    "            bins=bins,\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "\n",
    "    # ---------- client mean / max lines -------------------------------------\n",
    "    ax.axvline(c_extreme, color=\"red\", ls=\"--\", lw=2, label=\"Client Max\")\n",
    "    ax.axvline(c_mean, color=\"violet\", ls=\"--\", lw=2, label=\"Client Mean\")\n",
    "\n",
    "    # ---------- cosmetics ----------------------------------------------------\n",
    "    ax.set_xlabel(variable.replace(\"_\", \" \"), color=\"slategrey\")\n",
    "    ax.set_ylabel(\"Frequency\", color=\"slategrey\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"lightgrey\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"lightgrey\")\n",
    "    ax.grid(color=\"dimgrey\")\n",
    "\n",
    "    txt = (\n",
    "        f\"Percentile (mean): {perc_mean:.1f}%\"\n",
    "        f\"\\nMean: {c_mean:.2f}\"\n",
    "        f\"\\nMax:  {c_extreme:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        0.95,\n",
    "        0.05,\n",
    "        txt,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax.transAxes,\n",
    "        color=\"white\",\n",
    "        fontsize=9,\n",
    "        backgroundcolor=\"#181818\",\n",
    "    )\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "\n",
    "    # ---------- save ---------------------------------------------------------\n",
    "    out_path = os.path.join(tmpdirname, f\"{variable}_histogram.png\")\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\", facecolor=\"#181818\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Function to generate scatter plot for CMJ\n",
    "def generate_scatter_plot(client_data, reference_data, x_var, y_var, title, tmpdirname):\n",
    "    plt.figure(facecolor='#181818', figsize=(6, 6))\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    # Create scatter plot for reference data (cornflower blue)\n",
    "    ax.scatter(reference_data[x_var], reference_data[y_var], label='Reference', alpha=0.5, color='cornflowerblue')\n",
    "\n",
    "    # Create scatter plot for client data (red)\n",
    "    ax.scatter(client_data[x_var], client_data[y_var], label='Client', color='red', edgecolors='black', s=100)\n",
    "\n",
    "    # Set axis labels, replacing underscores with spaces\n",
    "    ax.set_xlabel(x_var.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel(y_var.replace('_', ' '), color='slategrey')\n",
    "\n",
    "    # Dynamically set ticks and numbers to light grey\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "\n",
    "    # Add vertical and horizontal reference lines (light grey)\n",
    "    ax.axvline(x=np.mean(reference_data[x_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=np.mean(reference_data[y_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize grid style\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(facecolor='black', edgecolor='grey', prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    # Save scatter plot to file\n",
    "    scatter_filename = os.path.join(tmpdirname, 'cmj_scatter.png')\n",
    "    plt.savefig(scatter_filename, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_filename\n",
    "\n",
    "\n",
    "def load_power_txt(txt_path: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse exported power file (like your example). Returns a pandas Series of power.\n",
    "    Assumes: header lines, then a line starting with 'ITEM', then data rows:\n",
    "             <index>\\t<value>\n",
    "    Skips rows without a numeric second field.\n",
    "    \"\"\"\n",
    "    power_vals = []\n",
    "    in_data = False\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not in_data:\n",
    "                if line.startswith(\"ITEM\"):\n",
    "                    in_data = True\n",
    "                continue\n",
    "            # from here on, try to parse the last field as a float\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = re.split(r\"\\t+\", line)\n",
    "            if len(parts) < 2:\n",
    "                # sometimes a blank power cell exists on the first data row — skip it\n",
    "                continue\n",
    "            try:\n",
    "                val = float(parts[-1])\n",
    "                power_vals.append(val)\n",
    "            except ValueError:\n",
    "                # non-numeric tail → ignore\n",
    "                continue\n",
    "    if not power_vals:\n",
    "        raise ValueError(f\"No power values parsed from {txt_path}\")\n",
    "    return pd.Series(power_vals, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Compute useful shape/temporal features.\n",
    "    power: Series of power (W). fs_hz: sampling rate (Hz). If unknown, 1000 Hz is typical.\n",
    "    Returns a dict of metrics.\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # basic\n",
    "    p_peak_idx = int(np.nanargmax(p))\n",
    "    p_peak     = float(p[p_peak_idx])\n",
    "    t_peak     = float(t[p_peak_idx])\n",
    "\n",
    "    # onset/offset via % of peak (robust to baseline drift)\n",
    "    thr10 = 0.10 * p_peak\n",
    "    thr50 = 0.50 * p_peak\n",
    "    thr90 = 0.90 * p_peak\n",
    "\n",
    "    # first index above 10% of peak\n",
    "    try:\n",
    "        onset_idx = int(np.argmax(p >= thr10))\n",
    "    except ValueError:\n",
    "        onset_idx = 0\n",
    "    # first index after peak that falls below 10% (or end)\n",
    "    post = p[p_peak_idx:]\n",
    "    off_rel = np.argmax(post < thr10) if np.any(post < thr10) else (post.size - 1)\n",
    "    offset_idx = p_peak_idx + int(off_rel)\n",
    "\n",
    "    # 10–90% rise time on rising limb\n",
    "    rising = p[:p_peak_idx+1]\n",
    "    try:\n",
    "        i10 = int(np.argmax(rising >= thr10))\n",
    "        i90 = int(np.argmax(rising >= thr90))\n",
    "        rise_time = (i90 - i10) / fs_hz if i90 > i10 else np.nan\n",
    "        rise_slope = (0.8 * p_peak) / rise_time if rise_time and rise_time > 0 else np.nan\n",
    "    except ValueError:\n",
    "        i10 = i90 = None\n",
    "        rise_time = np.nan\n",
    "        rise_slope = np.nan\n",
    "\n",
    "    # FWHM (50% of peak) width\n",
    "    # left crossing\n",
    "    try:\n",
    "        left_idx  = int(np.argmax(rising >= thr50))\n",
    "    except ValueError:\n",
    "        left_idx = p_peak_idx\n",
    "    # right crossing\n",
    "    falling = p[p_peak_idx:]\n",
    "    try:\n",
    "        right_rel = int(np.argmax(falling <= thr50))\n",
    "        right_idx = p_peak_idx + right_rel\n",
    "    except ValueError:\n",
    "        right_idx = p_peak_idx\n",
    "    fwhm_sec = (right_idx - left_idx) / fs_hz if right_idx > left_idx else np.nan\n",
    "\n",
    "    # Work/impulse of power (area under curve) over the active window\n",
    "    a = max(0, onset_idx)\n",
    "    b = min(n - 1, max(offset_idx, p_peak_idx))\n",
    "    auc_joules = float(np.trapezoid(np.nan_to_num(p[a:b+1], nan=0.0), dx=1.0/fs_hz))\n",
    "\n",
    "    # timing “balance”: center of mass of power curve (0..1)\n",
    "    # (earlier vs. later power concentration)\n",
    "    weights = p[a:b+1].clip(min=0)\n",
    "    if weights.sum() > 0:\n",
    "        t_window = t[a:b+1]\n",
    "        t_com = float(np.sum(t_window * weights) / np.sum(weights))\n",
    "        t_com_norm = (t_com - t[a]) / max(1e-9, (t[b] - t[a]))\n",
    "    else:\n",
    "        t_com = np.nan\n",
    "        t_com_norm = np.nan\n",
    "\n",
    "    # variability around peak (local coefficient of variation in ±50 ms)\n",
    "    w = int(0.05 * fs_hz)\n",
    "    lo = max(0, p_peak_idx - w)\n",
    "    hi = min(n, p_peak_idx + w + 1)\n",
    "    local = p[lo:hi]\n",
    "    cv_local = float(np.std(local) / np.mean(local)) if np.mean(local) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_samples\": n,\n",
    "        \"fs_hz\": fs_hz,\n",
    "        \"peak_power_w\": p_peak,\n",
    "        \"time_to_peak_s\": t_peak,\n",
    "        \"rise_time_10_90_s\": float(rise_time),\n",
    "        \"rise_slope_w_per_s\": float(rise_slope),\n",
    "        \"fwhm_s\": float(fwhm_sec),\n",
    "        \"auc_j\": auc_joules,\n",
    "        \"onset_idx\": a,\n",
    "        \"offset_idx\": b,\n",
    "        \"peak_idx\": p_peak_idx,\n",
    "        \"t_com_s\": t_com,\n",
    "        \"t_com_norm_0to1\": t_com_norm,\n",
    "        \"cv_local_peak\": cv_local,\n",
    "        \"i10_idx\": int(i10) if isinstance(i10, int) else None,\n",
    "        \"i90_idx\": int(i90) if isinstance(i90, int) else None,\n",
    "        \"left50_idx\": left_idx,\n",
    "        \"right50_idx\": right_idx,\n",
    "    }\n",
    "\n",
    "def plot_power_curve(power: pd.Series,\n",
    "                     metrics: dict,\n",
    "                     out_path: str,\n",
    "                     title: str = \"Power Curve\",\n",
    "                     annotate: bool = True):\n",
    "    \"\"\"\n",
    "    Plot power vs. time with annotations (peak, 10–90 rise, FWHM, AUC window).\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    t = np.arange(p.size) / metrics[\"fs_hz\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    ax.plot(t, p, lw=2, label=\"Power\")\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "\n",
    "    # annotate regions\n",
    "    a, b = metrics[\"onset_idx\"], metrics[\"offset_idx\"]\n",
    "    ax.axvspan(t[a], t[b], color=\"white\", alpha=0.07, label=\"active window\")\n",
    "\n",
    "    # peak\n",
    "    pk = metrics[\"peak_idx\"]\n",
    "    ax.plot([t[pk]], [p[pk]], \"o\", ms=6, color=\"tomato\", label=\"Peak\")\n",
    "    ax.axhline(p[pk]*0.5, ls=\"--\", lw=1, color=\"grey\")\n",
    "    ax.vlines([t[metrics[\"left50_idx\"]], t[metrics[\"right50_idx\"]]],\n",
    "              ymin=0, ymax=p[pk]*0.5, linestyles=\"--\", colors=\"grey\", lw=1)\n",
    "\n",
    "    # 10–90 rise (if available)\n",
    "    if metrics[\"i10_idx\"] is not None and metrics[\"i90_idx\"] is not None:\n",
    "        ax.plot([t[metrics[\"i10_idx\"]], t[metrics[\"i90_idx\"]]],\n",
    "                [p[metrics[\"i10_idx\"]], p[metrics[\"i90_idx\"]]],\n",
    "                lw=3, color=\"deepskyblue\", label=\"10–90% rise\")\n",
    "\n",
    "    if annotate:\n",
    "        txt = (f\"Peak: {metrics['peak_power_w']:.1f} W @ {metrics['time_to_peak_s']:.3f} s\"\n",
    "               f\"\\nRise 10–90: {metrics['rise_time_10_90_s']:.3f} s\"\n",
    "               f\"\\nFWHM: {metrics['fwhm_s']:.3f} s\"\n",
    "               f\"\\nWork (AUC): {metrics['auc_j']:.1f} J\"\n",
    "               f\"\\nTiming COM: {metrics['t_com_norm_0to1']:.2f} (0 early…1 late)\")\n",
    "        ax.text(0.99, 0.02, txt, ha=\"right\", va=\"bottom\",\n",
    "                transform=ax.transAxes, fontsize=9, color=\"white\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"#181818\", edgecolor=\"#444\"))\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def overlay_power_trials(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         out_path: str,\n",
    "                         title: str = \"Power (all trials)\",\n",
    "                         align: str = \"peak\",          # \"peak\", \"onset10\", or \"none\"\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40),\n",
    "                         show_mean: bool = True):\n",
    "    \"\"\"\n",
    "    Overlay multiple power traces and align them in time.\n",
    "\n",
    "    align:\n",
    "      - \"peak\"    → align each trial's max power to t=0\n",
    "      - \"onset10\" → align first sample ≥ 10% of that trial's peak to t=0\n",
    "      - \"none\"    → no alignment; left edges at t=0\n",
    "\n",
    "    window_s: (pre, post) seconds to show around t=0 (None to show full extent)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- convert input to arrays; find alignment index per trial --------------\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"overlay_power_trials: no non-empty traces provided\")\n",
    "\n",
    "    def _align_index(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        # guard against NaNs\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        # peak index\n",
    "        try:\n",
    "            pk = int(np.nanargmax(x))\n",
    "        except ValueError:\n",
    "            pk = 0\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            # first index >= 10% of that trial's peak\n",
    "            idx = int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "            return idx\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    align_idx = [ _align_index(x) for x in arrs ]\n",
    "\n",
    "    # --- build a padded matrix so all alignment points land at the same column\n",
    "    max_left  = max(align_idx)                                  # largest left padding needed\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, align_idx)]   # samples from align idx to end\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right                                     # total aligned length\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, align_idx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    # --- time vector: t=0 at the common alignment column ---------------------\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    # --- optional cropping around t=0 ----------------------------------------\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        aligned = aligned[:, i_lo:i_hi]\n",
    "        t = t[i_lo:i_hi]\n",
    "        \n",
    "    # --- plot ----------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    \n",
    "    # consistent y-limits (same scale across trials)\n",
    "    y_max = np.nanmax(aligned)\n",
    "    y_min = np.nanmin(aligned)\n",
    "    \n",
    "    # NEW: mask-based plotting so left/right NaNs don't truncate the curve visually\n",
    "    coverage_flags = []  # we'll use this to warn about very short traces\n",
    "    for i in range(aligned.shape[0]):\n",
    "        row = aligned[i]\n",
    "        mask = np.isfinite(row)\n",
    "        cov = np.count_nonzero(mask) / max(1, len(row))\n",
    "        coverage_flags.append(cov)\n",
    "        if np.count_nonzero(mask) >= 2:\n",
    "            ax.plot(t[mask], row[mask], lw=1.2, alpha=0.6)\n",
    "    \n",
    "    # optional: highlight which trials are very short (<50% of the window)\n",
    "    short = [idx for idx, c in enumerate(coverage_flags, start=1) if c < 0.5]\n",
    "    if short:\n",
    "        print(f\"overlay_power_trials: {len(short)} trial(s) with <50% coverage in the window: {short}. \"\n",
    "              f\"Consider changing window_s or checking the export files.\")\n",
    "    \n",
    "    if show_mean:\n",
    "        mean_curve = np.nanmean(aligned, axis=0)\n",
    "        ax.plot(t, mean_curve, lw=2.2, color=\"cyan\", label=\"Mean\")\n",
    "    \n",
    "    # vertical line at alignment point\n",
    "    ax.axvline(0.0, color=\"grey\", lw=1, ls=\"--\",\n",
    "               label=(\"Aligned peak\" if align == \"peak\" else \"Aligned onset\"))\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s, aligned)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "    if show_mean:\n",
    "        ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    \n",
    "    ax.set_ylim(y_min - 0.05 * abs(y_max - y_min), y_max + 0.05 * abs(y_max - y_min))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def find_power_files(movement: str,\n",
    "                     base_dir: str = r'D:\\Athletic Screen 2.0\\Output Files') -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds exported power files for a movement. Supports:\n",
    "      CMJ_Power.txt, CMJ1_Power.txt, CMJ2_Power.txt, ...\n",
    "      DJ_Power.txt,  DJ*_Power.txt\n",
    "      SLV_Power.txt, SLV*_Power.txt\n",
    "    Returns a sorted (unique) list of full paths.\n",
    "    \"\"\"\n",
    "    pats = [f\"{movement}_Power.txt\", f\"{movement}*_Power.txt\"]\n",
    "    files = []\n",
    "    for p in pats:\n",
    "        files += globmod.glob(os.path.join(base_dir, p))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def build_aligned_matrix(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         align: str = \"peak\",\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40)):\n",
    "    \"\"\"\n",
    "    Returns (aligned 2D array [n_trials x T], time vector [T], indices_info[list]).\n",
    "    Time is centered so the chosen alignment point is at t=0.\n",
    "    \"\"\"\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"build_aligned_matrix: no non-empty traces\")\n",
    "\n",
    "    def _align_idx(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        pk = int(np.nanargmax(x))\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            return int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "        return 0\n",
    "\n",
    "    aidx = [_align_idx(x) for x in arrs]\n",
    "    max_left = max(aidx)\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, aidx)]\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, aidx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        return aligned[:, i_lo:i_hi], t[i_lo:i_hi], aidx\n",
    "    return aligned, t, aidx\n",
    "\n",
    "def mean_aligned_curve(traces: list[pd.Series],\n",
    "                       fs_hz: float,\n",
    "                       align: str = \"peak\",\n",
    "                       window_s: tuple[float, float] | None = (0.30, 0.40)) -> pd.Series:\n",
    "    aligned, t, _ = build_aligned_matrix(traces, fs_hz, align, window_s)\n",
    "    mean_curve = np.nanmean(aligned, axis=0)\n",
    "    return pd.Series(mean_curve, index=t, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve_advanced(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Extends your analyze_power_curve with additional, practical features:\n",
    "      • rpd_max (max rate of power development) & time to RPDmax\n",
    "      • AUC early (pre-peak) / late (post-peak), % early work\n",
    "      • decay_90_10 on falling limb\n",
    "      • skewness, kurtosis\n",
    "      • spectral centroid (how ‘fast’ the curve is in frequency domain)\n",
    "    \"\"\"\n",
    "    base = analyze_power_curve(power, fs_hz)\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # RPD\n",
    "    dp = np.gradient(p, 1.0/fs_hz)\n",
    "    rpd_max = float(np.nanmax(dp))\n",
    "    rpd_idx = int(np.nanargmax(dp))\n",
    "    base[\"rpd_max_w_per_s\"]   = rpd_max\n",
    "    base[\"time_to_rpd_max_s\"] = rpd_idx / fs_hz\n",
    "\n",
    "    # Early/late work around peak (use on/peak/off from base)\n",
    "    a, b, pk = base[\"onset_idx\"], base[\"offset_idx\"], base[\"peak_idx\"]\n",
    "    auc_pre  = float(np.trapezoid(np.nan_to_num(p[a:pk+1],  nan=0.0), dx=1.0/fs_hz)) if pk >= a else np.nan\n",
    "    auc_post = float(np.trapezoid(np.nan_to_num(p[pk:b+1], nan=0.0), dx=1.0/fs_hz)) if b >= pk else np.nan\n",
    "    total    = (auc_pre if np.isfinite(auc_pre) else 0) + (auc_post if np.isfinite(auc_post) else 0)\n",
    "    base[\"auc_pre_j\"]      = auc_pre\n",
    "    base[\"auc_post_j\"]     = auc_post\n",
    "    base[\"work_early_pct\"] = float(100.0 * auc_pre / total) if total > 0 else np.nan\n",
    "\n",
    "    # Decay time 90→10% of peak on falling limb\n",
    "    peak_val = p[pk]\n",
    "    fall = p[pk:]\n",
    "    thr90 = 0.90 * peak_val\n",
    "    thr10 = 0.10 * peak_val\n",
    "    i90 = int(np.argmax(fall <= thr90)) if np.any(fall <= thr90) else 0\n",
    "    i10 = int(np.argmax(fall <= thr10)) if np.any(fall <= thr10) else len(fall)-1\n",
    "    base[\"decay_90_10_s\"] = (i10 - i90) / fs_hz if i10 > i90 else np.nan\n",
    "\n",
    "    # Shape stats\n",
    "    finite = np.isfinite(p)\n",
    "    base[\"skewness\"] = float(stats.skew(p[finite])) if np.any(finite) else np.nan\n",
    "    base[\"kurtosis\"] = float(stats.kurtosis(p[finite], fisher=True)) if np.any(finite) else np.nan\n",
    "\n",
    "    # Spectral centroid\n",
    "    x = p - np.nanmean(p)\n",
    "    X = np.abs(np.fft.rfft(np.nan_to_num(x)))\n",
    "    freqs = np.fft.rfftfreq(x.size, d=1.0/fs_hz)\n",
    "    base[\"spectral_centroid_hz\"] = float(np.sum(freqs * X) / max(1e-12, np.sum(X)))\n",
    "\n",
    "    return base\n",
    "\n",
    "def add_power_analysis_section(doc: Document,\n",
    "                               movement: str,\n",
    "                               traces: list[pd.Series],\n",
    "                               fs_hz: float,\n",
    "                               tmpdirname: str,\n",
    "                               reference_cursor,\n",
    "                               reference_table: str):\n",
    "    \"\"\"\n",
    "    1) Overlay (peaks aligned) + mean curve\n",
    "    2) Annotated mean power curve\n",
    "    3) Table of per-trial metrics + Mean/SD\n",
    "    4) Adds reference percentile for peak power (vs PP_FORCEPLATE in ref DB)\n",
    "    \"\"\"\n",
    "    # --- 1) Overlay (aligned at peak) ---------------------------------------\n",
    "    overlay_png = os.path.join(tmpdirname, f\"{movement}_power_overlay.png\")\n",
    "    overlay_power_trials(\n",
    "        traces, fs_hz=fs_hz, out_path=overlay_png,\n",
    "        title=f\"{movement} Power – All Trials (peaks aligned)\",\n",
    "        align=\"peak\", window_s=(0.30, 0.40), show_mean=True\n",
    "    )\n",
    "    doc.add_paragraph(\"Power Curves (aligned at peak)\", style=\"Heading 2\")\n",
    "    doc.add_picture(overlay_png, width=Inches(6))\n",
    "\n",
    "    # --- 2) Mean curve (aligned) + annotated plot ---------------------------\n",
    "    mean_series = mean_aligned_curve(traces, fs_hz, align=\"peak\", window_s=(0.30, 0.40))\n",
    "    mean_metrics = analyze_power_curve_advanced(mean_series, fs_hz=fs_hz)\n",
    "\n",
    "    mean_png = os.path.join(tmpdirname, f\"{movement}_power_mean_annotated.png\")\n",
    "    plot_power_curve(mean_series, mean_metrics, mean_png,\n",
    "                     title=f\"{movement} – Mean Power (aligned)\")\n",
    "    doc.add_paragraph(\"Mean Power Curve (annotated)\", style=\"Heading 3\")\n",
    "    doc.add_picture(mean_png, width=Inches(6))\n",
    "\n",
    "    # --- 3) Per-trial metrics & summary -------------------------------------\n",
    "    per = [analyze_power_curve_advanced(s, fs_hz=fs_hz) for s in traces]\n",
    "    df  = pd.DataFrame(per)\n",
    "\n",
    "    # Pick the important rows for the document (order here = row order in table)\n",
    "    metric_rows = [\n",
    "        (\"Peak Power (W)\",            \"peak_power_w\",        \"{:.0f}\"),\n",
    "        (\"Time to Peak (s)\",          \"time_to_peak_s\",      \"{:.3f}\"),\n",
    "        (\"RPD max (W/s)\",             \"rpd_max_w_per_s\",     \"{:.0f}\"),\n",
    "        (\"Time to RPD max (s)\",       \"time_to_rpd_max_s\",   \"{:.3f}\"),\n",
    "        (\"Rise 10–90% (s)\",           \"rise_time_10_90_s\",   \"{:.3f}\"),\n",
    "        (\"FWHM (s)\",                  \"fwhm_s\",              \"{:.3f}\"),\n",
    "        (\"Work (AUC, J)\",             \"auc_j\",               \"{:.0f}\"),\n",
    "        (\"Early work (%)\",            \"work_early_pct\",      \"{:.1f}\"),\n",
    "        (\"Decay 90→10% (s)\",          \"decay_90_10_s\",       \"{:.3f}\"),\n",
    "        (\"Timing CoM (0…1)\",          \"t_com_norm_0to1\",     \"{:.2f}\"),\n",
    "        (\"Skewness\",                  \"skewness\",            \"{:.2f}\"),\n",
    "        (\"Kurtosis\",                  \"kurtosis\",            \"{:.2f}\"),\n",
    "        (\"Spectral centroid (Hz)\",    \"spectral_centroid_hz\",\"{:.2f}\"),\n",
    "    ]\n",
    "\n",
    "    # table: Metric | Trial1 | Trial2 | ... | Mean | SD\n",
    "    tbl = doc.add_table(rows=1 + len(metric_rows), cols=2 + len(traces))\n",
    "    tbl.style = \"Light List\" if \"Light List\" in [s.name for s in doc.styles] else tbl.style\n",
    "    # header\n",
    "    hdr = tbl.rows[0].cells\n",
    "    hdr[0].text = \"Metric\"\n",
    "    for i in range(len(traces)):\n",
    "        hdr[1+i].text = f\"Trial {i+1}\"\n",
    "    hdr[-1].text = \"Mean ± SD\"\n",
    "\n",
    "    # body\n",
    "    for r, (label, key, fmt) in enumerate(metric_rows, start=1):\n",
    "        row_cells = tbl.rows[r].cells\n",
    "        row_cells[0].text = label\n",
    "        vals = df.get(key, pd.Series([np.nan]*len(traces))).values\n",
    "        for i, v in enumerate(vals):\n",
    "            row_cells[1+i].text = (fmt.format(v) if np.isfinite(v) else \"—\")\n",
    "        mu = np.nanmean(vals)\n",
    "        sd = np.nanstd(vals, ddof=1) if np.count_nonzero(np.isfinite(vals)) > 1 else np.nan\n",
    "        row_cells[-1].text = (f\"{fmt.format(mu)} ± {fmt.format(sd)}\"\n",
    "                              if np.isfinite(mu) and np.isfinite(sd) else\n",
    "                              (fmt.format(mu) if np.isfinite(mu) else \"—\"))\n",
    "\n",
    "    # small spacer\n",
    "    doc.add_paragraph(\"\")\n",
    "\n",
    "    # --- 4) Reference percentile for peak power (vs. DB PP_FORCEPLATE) ------\n",
    "    try:\n",
    "        reference_cursor.execute(f\"SELECT PP_FORCEPLATE FROM {reference_table} WHERE PP_FORCEPLATE IS NOT NULL\")\n",
    "        ref_pp = np.array([r[0] for r in reference_cursor.fetchall()], dtype=float)\n",
    "    except sqlite3.OperationalError:\n",
    "        ref_pp = np.array([])\n",
    "\n",
    "    if ref_pp.size:\n",
    "        # use the highest peak among trials (or mean peak if you prefer)\n",
    "        trial_peaks = df[\"peak_power_w\"].values\n",
    "        best_peak   = float(np.nanmax(trial_peaks)) if trial_peaks.size else np.nan\n",
    "        pctl        = percentile_vs_reference(best_peak, ref_pp) if np.isfinite(best_peak) else np.nan\n",
    "        doc.add_paragraph(f\"Reference percentile (peak power): {pctl:.1f}%\", style=\"Intense Quote\")\n",
    "\n",
    "def percentile_vs_reference(value: float, reference_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Wrapper for percentile (1..99) using scipy-like methodology.\n",
    "    \"\"\"\n",
    "    return stats.percentileofscore(reference_values, value)\n",
    "\n",
    "# Modified function to generate a histogram comparing left and right leg data\n",
    "def generate_slv_histogram(variable, left_value, right_value,\n",
    "                           reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ green    = client LEFT (latest trial value you passed in)\n",
    "    ─ orange   = client RIGHT\n",
    "\n",
    "    Text box shows, for each side:\n",
    "        • mean across all trials from the same assessment day\n",
    "        • max across those trials\n",
    "        • percentile of that mean vs. the reference distribution\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, re, sqlite3\n",
    "\n",
    "    # ── helper – get all trials for a given side on the same assessment day ──\n",
    "    def _fetch_side_vals(side):\n",
    "        # detect yyyy-mm-dd prefix in the first trial_name for this side\n",
    "        client_cursor.execute(\n",
    "            \"SELECT trial_name FROM SLV WHERE name=? AND side=? LIMIT 1\",\n",
    "            (client_name, side)\n",
    "        )\n",
    "        row = client_cursor.fetchone()\n",
    "        date_prefix = None\n",
    "        if row and row[0]:\n",
    "            m = re.match(r'(\\d{4}[-_]\\d{2}[-_]\\d{2})', row[0])\n",
    "            date_prefix = m.group(1) if m else None\n",
    "\n",
    "        if date_prefix:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=? AND trial_name LIKE ?\"\n",
    "            client_cursor.execute(q, (client_name, side, f'{date_prefix}%'))\n",
    "        else:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=?\"\n",
    "            client_cursor.execute(q, (client_name, side))\n",
    "\n",
    "        return [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "\n",
    "    # pull all trials for each side; fall back to the single value passed in\n",
    "    left_vals  = np.asarray(_fetch_side_vals('Left')  or [left_value],  dtype=float)\n",
    "    right_vals = np.asarray(_fetch_side_vals('Right') or [right_value], dtype=float)\n",
    "\n",
    "    left_mean,  left_max  = left_vals.mean(),  left_vals.max()\n",
    "    right_mean, right_max = right_vals.mean(), right_vals.max()\n",
    "\n",
    "    left_pct  = calculate_percentile(left_mean,  reference_data)\n",
    "    right_pct = calculate_percentile(right_mean, reference_data)\n",
    "\n",
    "    # ── plot ────────────────────────────────────────────────────────────────\n",
    "    plt.figure(facecolor='#181818')\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    ax.hist(reference_data, bins=20, color='cornflowerblue',\n",
    "            alpha=0.7, edgecolor='white', label='Reference')\n",
    "\n",
    "    ax.axvline(left_value,  color='green',  ls='--', lw=2, label='Left (latest)')\n",
    "    ax.axvline(right_value, color='orange', ls='--', lw=2, label='Right (latest)')\n",
    "\n",
    "    ax.set_xlabel(variable.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel('Frequency',               color='slategrey')\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    txt = (\n",
    "        f'LEFT  – mean: {left_mean:.2f}\\n'\n",
    "        f'        max:  {left_max:.2f}\\n'\n",
    "        f'        %ile: {left_pct:.1f}\\n'\n",
    "        f'RIGHT – mean: {right_mean:.2f}\\n'\n",
    "        f'        max:  {right_max:.2f}\\n'\n",
    "        f'        %ile: {right_pct:.1f}'\n",
    "    )\n",
    "    plt.text(0.95, 0.05, txt, ha='right', va='bottom',\n",
    "             transform=ax.transAxes, color='white', fontsize=9,\n",
    "             backgroundcolor='#181818')\n",
    "\n",
    "    ax.legend(facecolor='black', edgecolor='grey',\n",
    "              prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    out_path = os.path.join(tmpdirname, f'{variable}_histogram_slv.png')\n",
    "    plt.savefig(out_path, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Prepare the document\n",
    "doc = Document()\n",
    "doc.add_picture(\"8ctane Baseball - Black abd Blue BG.jpeg\", width=Inches(4.0))  # Replace with your logo path\n",
    "doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Adding player name and date\n",
    "doc.add_paragraph(f\"Player's Name: {client_name}\")  # Replace client_name with dynamic value\n",
    "doc.add_paragraph(f\"Date: {date.today().strftime('%B %d, %Y')}\")\n",
    "\n",
    "# Create a temporary directory to store images\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    # List of movements to process\n",
    "    movements = ['CMJ', 'PPU', 'DJ', 'SLV', 'NMT']\n",
    "    \n",
    "    for movement in movements:\n",
    "        # Add movement title\n",
    "        doc.add_paragraph(f\"{movement} Report\", style='Title')\n",
    "        doc.add_paragraph(f\"This section includes percentile reports and comparisons for {movement}.\", style='Heading 2')\n",
    "\n",
    "        if movement == 'CMJ':\n",
    "            # Fetch CMJ data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_cmj_data = client_cursor.fetchone()\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ\n",
    "            \"\"\")\n",
    "            reference_cmj_data = np.array(reference_cursor.fetchall())\n",
    "            \n",
    "            # --- CMJ power files & analysis ---\n",
    "            cmj_files = find_power_files(\"CMJ\")\n",
    "            if cmj_files:\n",
    "                cmj_traces = [load_power_txt(pf) for pf in cmj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"CMJ\", traces=cmj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"CMJ\"\n",
    "                )\n",
    "\n",
    "            # Ensure data exists before proceeding\n",
    "            if client_cmj_data and reference_cmj_data.size > 0:\n",
    "                # Generate bar graphs for each variable in CMJ\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                for i, var in enumerate(variables):\n",
    "                    # Format the variable name by removing underscores\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    \n",
    "                    # Add variable title before the graph\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    \n",
    "                    # Generate the bar graph and add to document\n",
    "                    bar_image = generate_bar_graph(var, client_cmj_data[i], reference_cmj_data[:, i], f'{formatted_var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "                \n",
    "                # Generate scatter plot for CMJ (Force_Peak_Power vs. Velo_Peak_Power)\n",
    "                client_cmj_dict = {'Force_at_PP': client_cmj_data[3],\n",
    "                   'Vel_at_PP':   client_cmj_data[4]}\n",
    "                reference_cmj_dict = pd.DataFrame(reference_cmj_data, columns=variables)\n",
    "                \n",
    "                # Add scatter plot title and image\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                scatter_image = generate_scatter_plot(client_cmj_dict, reference_cmj_dict,\n",
    "                                                      'Force_at_PP', 'Vel_at_PP',\n",
    "                                                      'CMJ: Force vs. Velocity', tmpdirname)\n",
    "                doc.add_picture(scatter_image, width=Inches(6))\n",
    "                \n",
    "            # ───────────────────────── PPU ─────────────────────────        \n",
    "            elif movement == 'PPU':\n",
    "                # Fetch PPU data for the client\n",
    "                client_cursor.execute(\"\"\"\n",
    "                    SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                           Force_at_PP, Vel_at_PP\n",
    "                    FROM PPU WHERE name = ?\n",
    "                \"\"\", (client_name,))\n",
    "                client_ppu_data = client_cursor.fetchone()\n",
    "    \n",
    "                # Reference pull for PPU\n",
    "                try:\n",
    "                    reference_cursor.execute(\"\"\"\n",
    "                        SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                               Force_at_PP, Vel_at_PP\n",
    "                        FROM PPU\n",
    "                    \"\"\")\n",
    "                    reference_ppu_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "                except sqlite3.OperationalError:\n",
    "                    reference_ppu_data = np.empty((0, 5), dtype=float)\n",
    "    \n",
    "                # --- PPU power files & analysis (if exported similarly to CMJ) ---\n",
    "                ppu_files = find_power_files(\"PPU\")   # expects PPU_Power*.txt pattern\n",
    "                if ppu_files:\n",
    "                    ppu_traces = [load_power_txt(pf) for pf in ppu_files]\n",
    "                    add_power_analysis_section(\n",
    "                        doc, movement=\"PPU\", traces=ppu_traces, fs_hz=1000,\n",
    "                        tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                        reference_table=\"PPU\"\n",
    "                    )\n",
    "    \n",
    "                # Graphs and scatter if data exists\n",
    "                if client_ppu_data is not None and reference_ppu_data.size > 0:\n",
    "                    variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                    for i, var in enumerate(variables):\n",
    "                        formatted_var = var.replace('_', ' ')\n",
    "                        doc.add_paragraph(f\"{formatted_var} Comparison (PPU)\", style='Heading 2')\n",
    "                        bar_image = generate_bar_graph(\n",
    "                            var,\n",
    "                            float(client_ppu_data[i]),\n",
    "                            reference_ppu_data[:, i],\n",
    "                            f'{formatted_var} Comparison (PPU)',\n",
    "                            tmpdirname\n",
    "                        )\n",
    "                        doc.add_picture(bar_image, width=Inches(6))\n",
    "    \n",
    "                    # Force vs Velocity scatter for PPU\n",
    "                    client_ppu_dict = {'Force_at_PP': client_ppu_data[3],\n",
    "                                       'Vel_at_PP'  : client_ppu_data[4]}\n",
    "                    reference_ppu_df = pd.DataFrame(reference_ppu_data, columns=variables)\n",
    "                    doc.add_paragraph(\"Force vs. Velocity Scatter Plot (PPU)\", style='Heading 2')\n",
    "                    ppu_scatter = generate_scatter_plot(\n",
    "                        client_ppu_dict,\n",
    "                        reference_ppu_df if not reference_ppu_df.empty else pd.DataFrame(columns=variables),\n",
    "                        'Force_at_PP', 'Vel_at_PP',\n",
    "                        'PPU: Force vs. Velocity', tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(ppu_scatter, width=Inches(6))\n",
    "                else:\n",
    "                    print(\"PPU: missing client or reference data; skipping PPU figures.\")\n",
    "\n",
    "        # ───────────────────────── DJ ─────────────────────────        \n",
    "        elif movement == 'DJ':\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP, CT, RSI\n",
    "                FROM DJ\n",
    "                WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_dj_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (robust: pandas + print counts) ─────────────────────\n",
    "            dj_vars = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg',\n",
    "                       'Force_at_PP', 'Vel_at_PP', 'CT', 'RSI']\n",
    "            ref_sql = f\"SELECT {', '.join(dj_vars)} FROM DJ\"\n",
    "            reference_dj_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"DJ reference rows: {len(reference_dj_df)}\")\n",
    "            reference_dj_data = reference_dj_df.to_numpy(dtype=float, copy=False)\n",
    "\n",
    "            # ── POWER OVERLAY (files like *DJ*_Power*.txt or DJ_Power*.txt) ───\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "            dj_power_files = (sorted(globmod.glob(os.path.join(power_dir, \"*DJ*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"DJ_Power*.txt\"))))\n",
    "            # --- DJ power files & analysis ---\n",
    "            dj_files = find_power_files(\"DJ\")\n",
    "            if dj_files:\n",
    "                dj_traces = [load_power_txt(pf) for pf in dj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"DJ\", traces=dj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"DJ\"\n",
    "                )\n",
    "\n",
    "            # ── BAR GRAPHS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_dj_data:\n",
    "                for i, var in enumerate(dj_vars):\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison\", style='Heading 2')\n",
    "                    ref_col = reference_dj_data[:, i] if len(reference_dj_df) else np.array([])\n",
    "                    bar = generate_bar_graph(var, client_dj_data[i], ref_col,\n",
    "                                             f'{var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar, width=Inches(6))\n",
    "\n",
    "                client_dj_dict = {'Force_at_PP': client_dj_data[3],\n",
    "                                  'Vel_at_PP'  : client_dj_data[4]}\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                dj_scatter = generate_scatter_plot(\n",
    "                    client_dj_dict,\n",
    "                    reference_dj_df if not reference_dj_df.empty else pd.DataFrame(columns=dj_vars),\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'DJ: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(dj_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"No DJ client row found.\")\n",
    "\n",
    "        # ───────────────────────── SLV ───────────────────────        \n",
    "        elif movement == 'SLV':\n",
    "            # ── CLIENT ROWS (Left/Right) ───────────────────────────────────────\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Left'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvl_data = client_cursor.fetchone()\n",
    "\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Right'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvr_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (keep side to filter/inspect if needed) ─────────────\n",
    "            slv_vars_no_side = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "            ref_sql = f\"SELECT side, {', '.join(slv_vars_no_side)} FROM SLV\"\n",
    "            reference_slv_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"SLV reference rows: {len(reference_slv_df)}\")\n",
    "\n",
    "            # ── POWER OVERLAYS (Left & Right) ─────────────────────────────────\n",
    "            # We try common patterns. Adjust to your actual export names if needed.\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "\n",
    "            # Left trials\n",
    "            slv_left_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVL*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Left*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Left*_Power*.txt\"))))\n",
    "\n",
    "            # Right trials\n",
    "            slv_right_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVR*_Power*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Right*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Right*_Power*.txt\"))))\n",
    "\n",
    "            # --- SLV power files & analysis ---\n",
    "            slv_files = find_power_files(\"SLV\")\n",
    "            if slv_files:\n",
    "                slv_traces = [load_power_txt(pf) for pf in slv_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"SLV\", traces=slv_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"SLV\"\n",
    "                )\n",
    "\n",
    "            # ── HISTOGRAMS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_slvl_data and client_slvr_data and not reference_slv_df.empty:\n",
    "                # Build name→value dicts so we don't rely on fragile positional indexes\n",
    "                slv_cols = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                client_slvl = dict(zip(slv_cols, map(float, client_slvl_data)))\n",
    "                client_slvr = dict(zip(slv_cols, map(float, client_slvr_data)))\n",
    "            \n",
    "                # We want JH, PP, Force@PP (fixed), and Vel@PP\n",
    "                for var in ['JH_IN', 'PP_FORCEPLATE', 'Force_at_PP', 'Vel_at_PP']:\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison (Left vs Right)\",\n",
    "                                      style='Heading 2')\n",
    "            \n",
    "                    # Reference distribution by column name\n",
    "                    ref_col = reference_slv_df[var].to_numpy(dtype=float, copy=False)\n",
    "            \n",
    "                    # Client values by column name (correct variables now)\n",
    "                    left_val  = client_slvl.get(var, np.nan)\n",
    "                    right_val = client_slvr.get(var, np.nan)\n",
    "            \n",
    "                    hist = generate_slv_histogram(\n",
    "                        var,\n",
    "                        left_val,            # Left\n",
    "                        right_val,           # Right\n",
    "                        ref_col,\n",
    "                        f'{var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(hist, width=Inches(6))\n",
    "            \n",
    "                # Scatter Force vs Velocity (merge both sides)\n",
    "                client_slv_all = np.array([list(client_slvl.values()),\n",
    "                                           list(client_slvr.values())], dtype=float)\n",
    "                client_slv_dict = {\n",
    "                    'Force_at_PP': [client_slvl['Force_at_PP'], client_slvr['Force_at_PP']],\n",
    "                    'Vel_at_PP'  : [client_slvl['Vel_at_PP'],   client_slvr['Vel_at_PP']],\n",
    "                }\n",
    "                ref_for_scatter = reference_slv_df[['Force_at_PP', 'Vel_at_PP']]\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                slv_scatter = generate_scatter_plot(\n",
    "                    client_slv_dict,\n",
    "                    ref_for_scatter,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'SLV: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(slv_scatter, width=Inches(6))\n",
    "            else:\n",
    "                if not (client_slvl_data and client_slvr_data):\n",
    "                    print(\"⚠️  Missing SLV client data – skipping SLV graphs.\")\n",
    "                if reference_slv_df.empty:\n",
    "                    print(\"⚠️  SLV reference pull returned 0 rows.\")\n",
    "    \n",
    "        elif movement == 'NMT':\n",
    "            # Fetch NMT data for the client (10s taps only)\n",
    "            print(\"Entering NMT section…\")\n",
    "            client_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT WHERE name = ?\", (client_name,))\n",
    "            client_nmt_data = client_cursor.fetchone()\n",
    "        \n",
    "            reference_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT\")\n",
    "            reference_nmt_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "        \n",
    "            print(f\"NMT client row present: {client_nmt_data is not None}, reference rows: {len(reference_nmt_data)}\")\n",
    "        \n",
    "            if client_nmt_data and reference_nmt_data.size > 0:\n",
    "                nmt_var_label = 'NUM TAPS (10s)'\n",
    "                doc.add_paragraph(f\"{nmt_var_label} Comparison\", style='Heading 2')\n",
    "        \n",
    "                # Pass table_hint=\"NMT\" to avoid mis-inference\n",
    "                nmt_image = generate_bar_graph(\n",
    "                    'NUM_TAPS_10s',\n",
    "                    float(client_nmt_data[0]),\n",
    "                    reference_nmt_data[:, 0],\n",
    "                    f'{nmt_var_label} Comparison',\n",
    "                    tmpdirname          # ← stop here\n",
    "                )\n",
    "                doc.add_picture(nmt_image, width=Inches(6))\n",
    "            else:\n",
    "                print(\"NMT: missing client or reference data; skipping figure.\")\n",
    "\n",
    "\n",
    "# Function to convert DOCX to images\n",
    "def docx_to_images(docx_path, output_dir):\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(docx_path)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Create a blank image with white background\n",
    "    img_width, img_height = 1000, 1500\n",
    "    image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Use a simple font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw the text onto the image\n",
    "    padding = 20\n",
    "    y_text = padding\n",
    "    for line in lines:\n",
    "        if y_text + padding > img_height:\n",
    "            # Save the image and start a new one if the text exceeds the page height\n",
    "            img_path = os.path.join(output_dir, f\"page_{int(y_text / img_height)}.png\")\n",
    "            image.save(img_path)\n",
    "            y_text = padding\n",
    "            image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # Calculate text size and draw it\n",
    "        text_bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        draw.text((padding, y_text), line, font=font, fill=\"black\")\n",
    "        y_text += text_height + padding\n",
    "\n",
    "    # Save the last image\n",
    "    img_path = os.path.join(output_dir, \"final_page.png\")\n",
    "    image.save(img_path)\n",
    "\n",
    "    return img_path\n",
    "\n",
    "doc.save(output_filename)          # ← only one final save\n",
    "print(f\"Document saved at: {output_filename}\")\n",
    "\n",
    "# Close connections\n",
    "client_conn.close()\n",
    "reference_conn.close()\n",
    "\n",
    "# Example usage\n",
    "img_output_directory = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Images'\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# Convert DOCX to images\n",
    "img_path = docx_to_images(output_filename, img_output_directory)\n",
    "print(f\"Images saved at {img_path}\")"
   ],
   "id": "88cef313dc0c2b36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases opened successfully.\n",
      "Client Name: Jalen Hollins\n",
      "Document saved at: G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Athletic_Report_Jalen_Hollins_All_Comp.docx\n",
      "Images saved at G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Images\\final_page.png\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "\n",
    "\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from datetime import date\n",
    "import tempfile\n",
    "import docx2txt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "# Corrected file paths with raw strings to handle backslashes properly\n",
    "client_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\movement_database_v2.db'\n",
    "reference_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\Athletic_Screen_All_data_v2.db'\n",
    "\n",
    "# Ensure the paths are valid and accessible\n",
    "if not os.path.exists(client_db_path):\n",
    "    print(f\"Client database not found at {client_db_path}\")\n",
    "if not os.path.exists(reference_db_path):\n",
    "    print(f\"Reference database not found at {reference_db_path}\")\n",
    "\n",
    "# Connect to the client and reference databases\n",
    "client_conn = sqlite3.connect(client_db_path)\n",
    "reference_conn = sqlite3.connect(reference_db_path)\n",
    "client_cursor = client_conn.cursor()\n",
    "reference_cursor = reference_conn.cursor()\n",
    "\n",
    "print(\"Databases opened successfully.\")\n",
    "\n",
    "# Fetch the client's name from the database (assuming the 'name' column is in all tables)\n",
    "client_cursor.execute(\"SELECT DISTINCT name FROM CMJ\")  # Change table if necessary\n",
    "client_name = client_cursor.fetchone()[0]  # Get the first row and first column\n",
    "print(f\"Client Name: {client_name}\")\n",
    "\n",
    "# ---------- build unique export paths (date-stamped, no overwrite) -----\n",
    "client_cursor.execute(\"SELECT MAX(date) FROM CMJ WHERE name = ?\", (client_name,))\n",
    "assessment_date = client_cursor.fetchone()[0]          # e.g. '2025-05-22'\n",
    "if not assessment_date:                                # fallback to today\n",
    "    assessment_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "reports_dir = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports'\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "parts = client_name.split(', ')\n",
    "client_name_rev = f\"{parts[1]}_{parts[0]}\" if len(parts) == 2 else client_name\n",
    "base_name = f\"Athletic_Report_{client_name_rev}_{assessment_date}_All Comp\"\n",
    "\n",
    "output_filename = os.path.join(reports_dir, base_name + \".docx\")\n",
    "img_output_directory = os.path.join(reports_dir, \"Images\", base_name)\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# auto-increment if the same file already exists\n",
    "counter = 1\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = os.path.join(\n",
    "        reports_dir, f\"{base_name}_{counter}.docx\"\n",
    "    )\n",
    "    img_output_directory = os.path.join(\n",
    "        reports_dir, \"Images\", f\"{base_name}_{counter}\"\n",
    "    )\n",
    "    os.makedirs(img_output_directory, exist_ok=True)\n",
    "    counter += 1\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Helper function to calculate percentile\n",
    "def calculate_percentile(value, reference_data):\n",
    "    return stats.percentileofscore(reference_data, value)\n",
    "\n",
    "# ─── UPDATED generate_bar_graph ────────────────────────────────────────────────\n",
    "def generate_bar_graph(variable, client_value, reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ red      = client MAX (best trial **within the same movement table**)\n",
    "    ─ violet   = client MEAN (average of those trials)\n",
    "\n",
    "    • No code outside this function needs to change.\n",
    "    • If the caller still passes one score, that is fine; this function looks\n",
    "      up any matching trials on the same assessment day and combines them.\n",
    "    • RSI histograms use 0.25-wide bins so you see bars at 1.25, 1.50, 1.75 …\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, sqlite3, re\n",
    "\n",
    "    # ───────────── lookup: which movement table are we dealing with? ─────────\n",
    "    column_table_map = {\"CMJ\": [\"CMJ\"], \"DJ\": [\"DJ\"], \"SLV\": [\"SLV\"], \"NMT\": [\"NMT\"]}\n",
    "    table_guess = None\n",
    "    for tbl in column_table_map:          # CMJ, DJ, SLV, NMT\n",
    "        try:\n",
    "            reference_cursor.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "            if reference_cursor.fetchone()[0] == len(reference_data):\n",
    "                table_guess = tbl\n",
    "                break\n",
    "        except sqlite3.OperationalError:\n",
    "            continue\n",
    "    if table_guess is None:               # fallback\n",
    "        for tbl in column_table_map:\n",
    "            try:\n",
    "                client_cursor.execute(f\"SELECT 1 FROM {tbl} LIMIT 1\")\n",
    "                table_guess = tbl\n",
    "                break\n",
    "            except sqlite3.OperationalError:\n",
    "                continue\n",
    "\n",
    "    # ───────────── gather all trials for this athlete / table / day ──────────\n",
    "    scores = []\n",
    "    if table_guess:\n",
    "        client_cursor.execute(\n",
    "            f\"SELECT trial_name FROM {table_guess} WHERE name=? LIMIT 1\",\n",
    "            (client_name,),\n",
    "        )\n",
    "        row = client_cursor.fetchone()\n",
    "        date_prefix = None\n",
    "        if row and row[0]:\n",
    "            m = re.match(r\"(\\d{4}[-_]\\d{2}[-_]\\d{2})\", row[0])\n",
    "            date_prefix = m.group(1) if m else None\n",
    "\n",
    "        if date_prefix:\n",
    "            q = f\"SELECT {variable} FROM {table_guess} WHERE name=? AND trial_name LIKE ?\"\n",
    "            client_cursor.execute(q, (client_name, f\"{date_prefix}%\"))\n",
    "        else:\n",
    "            q = f\"SELECT {variable} FROM {table_guess} WHERE name=?\"\n",
    "            client_cursor.execute(q, (client_name,))\n",
    "\n",
    "        scores = [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "\n",
    "    # fallback if still empty\n",
    "    if not scores:\n",
    "        scores = [client_value]\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    c_mean = scores.mean()\n",
    "    \n",
    "    # --- choose the extreme we draw as the red line ---------------------------\n",
    "    if variable.upper() == \"CT\":        # Contact-Time → use the LOWEST value\n",
    "        c_extreme = scores.min()\n",
    "        extreme_label = \"Client Min\"\n",
    "    else:                               # every other metric → highest value\n",
    "        c_extreme = scores.max()\n",
    "        extreme_label = \"Client Max\"\n",
    "\n",
    "    perc_mean = calculate_percentile(c_mean, reference_data)\n",
    "\n",
    "    # ──────────────────────────── plotting begins ────────────────────────────\n",
    "    plt.figure(facecolor=\"#181818\")\n",
    "    ax = plt.subplot(111, facecolor=\"#303030\")\n",
    "\n",
    "    reference_plotted = False  # flag to avoid double-plotting\n",
    "\n",
    "    # ---------- RSI special case: 0.25-wide bars & custom ticks -------------\n",
    "    if variable.upper() == \"RSI\":\n",
    "        lo = np.floor(reference_data.min() / 0.25) * 0.25\n",
    "        hi = np.ceil(reference_data.max() / 0.25) * 0.25\n",
    "        bins = np.arange(lo, hi + 0.25, 0.25)      # bin edges\n",
    "        centers = bins[:-1]                         # bar positions\n",
    "\n",
    "        counts, _ = np.histogram(reference_data, bins=bins)\n",
    "        ax.bar(\n",
    "            centers,\n",
    "            counts,\n",
    "            width=0.25,\n",
    "            align=\"edge\",\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "        # after counts, _ = np.histogram(...)\n",
    "        for x, h in zip(centers, counts):\n",
    "            if h == 0:                        # empty bin → draw thin outline\n",
    "                ax.bar(x, 1e-6, width=.25, align='edge',\n",
    "                       color='none', edgecolor='#404040', linewidth=.5)\n",
    "\n",
    "        ax.set_xticks(centers)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\" for x in centers], color=\"lightgrey\")\n",
    "\n",
    "        reference_plotted = True  # we already drew the reference bars\n",
    "    else:\n",
    "        bins = 20  # default bin count\n",
    "\n",
    "    # ---------- draw reference histogram when not plotted above -------------\n",
    "    if not reference_plotted:\n",
    "        ax.hist(\n",
    "            reference_data,\n",
    "            bins=bins,\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "\n",
    "    # ---------- client mean / max lines -------------------------------------\n",
    "    ax.axvline(c_extreme, color=\"red\", ls=\"--\", lw=2, label=\"Client Max\")\n",
    "    ax.axvline(c_mean, color=\"violet\", ls=\"--\", lw=2, label=\"Client Mean\")\n",
    "\n",
    "    # ---------- cosmetics ----------------------------------------------------\n",
    "    ax.set_xlabel(variable.replace(\"_\", \" \"), color=\"slategrey\")\n",
    "    ax.set_ylabel(\"Frequency\", color=\"slategrey\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"lightgrey\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"lightgrey\")\n",
    "    ax.grid(color=\"dimgrey\")\n",
    "\n",
    "    txt = (\n",
    "        f\"Percentile (mean): {perc_mean:.1f}%\"\n",
    "        f\"\\nMean: {c_mean:.2f}\"\n",
    "        f\"\\nMax:  {c_extreme:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        0.95,\n",
    "        0.05,\n",
    "        txt,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax.transAxes,\n",
    "        color=\"white\",\n",
    "        fontsize=9,\n",
    "        backgroundcolor=\"#181818\",\n",
    "    )\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "\n",
    "    # ---------- save ---------------------------------------------------------\n",
    "    out_path = os.path.join(tmpdirname, f\"{variable}_histogram.png\")\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\", facecolor=\"#181818\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Function to generate scatter plot for CMJ\n",
    "def generate_scatter_plot(client_data, reference_data, x_var, y_var, title, tmpdirname):\n",
    "    plt.figure(facecolor='#181818', figsize=(6, 6))\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    # Create scatter plot for reference data (cornflower blue)\n",
    "    ax.scatter(reference_data[x_var], reference_data[y_var], label='Reference', alpha=0.5, color='cornflowerblue')\n",
    "\n",
    "    # Create scatter plot for client data (red)\n",
    "    ax.scatter(client_data[x_var], client_data[y_var], label='Client', color='red', edgecolors='black', s=100)\n",
    "\n",
    "    # Set axis labels, replacing underscores with spaces\n",
    "    ax.set_xlabel(x_var.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel(y_var.replace('_', ' '), color='slategrey')\n",
    "\n",
    "    # Dynamically set ticks and numbers to light grey\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "\n",
    "    # Add vertical and horizontal reference lines (light grey)\n",
    "    ax.axvline(x=np.mean(reference_data[x_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=np.mean(reference_data[y_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize grid style\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(facecolor='black', edgecolor='grey', prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    # Save scatter plot to file\n",
    "    scatter_filename = os.path.join(tmpdirname, 'cmj_scatter.png')\n",
    "    plt.savefig(scatter_filename, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_filename\n",
    "\n",
    "# Modified function to generate a histogram comparing left and right leg data\n",
    "def generate_slv_histogram(variable, left_value, right_value,\n",
    "                           reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ green    = client LEFT (latest trial value you passed in)\n",
    "    ─ orange   = client RIGHT\n",
    "\n",
    "    Text box shows, for each side:\n",
    "        • mean across all trials from the same assessment day\n",
    "        • max across those trials\n",
    "        • percentile of that mean vs. the reference distribution\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, re, sqlite3\n",
    "\n",
    "    # ── helper – get all trials for a given side on the same assessment day ──\n",
    "    def _fetch_side_vals(side):\n",
    "        # detect yyyy-mm-dd prefix in the first trial_name for this side\n",
    "        client_cursor.execute(\n",
    "            \"SELECT trial_name FROM SLV WHERE name=? AND side=? LIMIT 1\",\n",
    "            (client_name, side)\n",
    "        )\n",
    "        row = client_cursor.fetchone()\n",
    "        date_prefix = None\n",
    "        if row and row[0]:\n",
    "            m = re.match(r'(\\d{4}[-_]\\d{2}[-_]\\d{2})', row[0])\n",
    "            date_prefix = m.group(1) if m else None\n",
    "\n",
    "        if date_prefix:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=? AND trial_name LIKE ?\"\n",
    "            client_cursor.execute(q, (client_name, side, f'{date_prefix}%'))\n",
    "        else:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=?\"\n",
    "            client_cursor.execute(q, (client_name, side))\n",
    "\n",
    "        return [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "\n",
    "    # pull all trials for each side; fall back to the single value passed in\n",
    "    left_vals  = np.asarray(_fetch_side_vals('Left')  or [left_value],  dtype=float)\n",
    "    right_vals = np.asarray(_fetch_side_vals('Right') or [right_value], dtype=float)\n",
    "\n",
    "    left_mean,  left_max  = left_vals.mean(),  left_vals.max()\n",
    "    right_mean, right_max = right_vals.mean(), right_vals.max()\n",
    "\n",
    "    left_pct  = calculate_percentile(left_mean,  reference_data)\n",
    "    right_pct = calculate_percentile(right_mean, reference_data)\n",
    "\n",
    "    # ── plot ────────────────────────────────────────────────────────────────\n",
    "    plt.figure(facecolor='#181818')\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    ax.hist(reference_data, bins=20, color='cornflowerblue',\n",
    "            alpha=0.7, edgecolor='white', label='Reference')\n",
    "\n",
    "    ax.axvline(left_value,  color='green',  ls='--', lw=2, label='Left (latest)')\n",
    "    ax.axvline(right_value, color='orange', ls='--', lw=2, label='Right (latest)')\n",
    "\n",
    "    ax.set_xlabel(variable.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel('Frequency',               color='slategrey')\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    txt = (\n",
    "        f'LEFT  – mean: {left_mean:.2f}\\n'\n",
    "        f'        max:  {left_max:.2f}\\n'\n",
    "        f'        %ile: {left_pct:.1f}\\n'\n",
    "        f'RIGHT – mean: {right_mean:.2f}\\n'\n",
    "        f'        max:  {right_max:.2f}\\n'\n",
    "        f'        %ile: {right_pct:.1f}'\n",
    "    )\n",
    "    plt.text(0.95, 0.05, txt, ha='right', va='bottom',\n",
    "             transform=ax.transAxes, color='white', fontsize=9,\n",
    "             backgroundcolor='#181818')\n",
    "\n",
    "    ax.legend(facecolor='black', edgecolor='grey',\n",
    "              prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    out_path = os.path.join(tmpdirname, f'{variable}_histogram_slv.png')\n",
    "    plt.savefig(out_path, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Prepare the document\n",
    "doc = Document()\n",
    "doc.add_picture(\"8ctane Baseball - Black abd Blue BG.jpeg\", width=Inches(4.0))  # Replace with your logo path\n",
    "doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Adding player name and date\n",
    "doc.add_paragraph(f\"Player's Name: {client_name}\")  # Replace client_name with dynamic value\n",
    "doc.add_paragraph(f\"Date: {date.today().strftime('%B %d, %Y')}\")\n",
    "\n",
    "# Create a temporary directory to store images\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    # List of movements to process\n",
    "    movements = ['CMJ', 'DJ', 'SLV', 'NMT']\n",
    "    \n",
    "    for movement in movements:\n",
    "        # Add movement title\n",
    "        doc.add_paragraph(f\"{movement} Report\", style='Title')\n",
    "        doc.add_paragraph(f\"This section includes percentile reports and comparisons for {movement}.\", style='Heading 2')\n",
    "\n",
    "        if movement == 'CMJ':\n",
    "            # Fetch CMJ data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_cmj_data = client_cursor.fetchone()\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ\n",
    "            \"\"\")\n",
    "            reference_cmj_data = np.array(reference_cursor.fetchall())\n",
    "            \n",
    "            # Ensure data exists before proceeding\n",
    "            if client_cmj_data and reference_cmj_data.size > 0:\n",
    "                # Generate bar graphs for each variable in CMJ\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                for i, var in enumerate(variables):\n",
    "                    # Format the variable name by removing underscores\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    \n",
    "                    # Add variable title before the graph\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    \n",
    "                    # Generate the bar graph and add to document\n",
    "                    bar_image = generate_bar_graph(var, client_cmj_data[i], reference_cmj_data[:, i], f'{formatted_var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "                \n",
    "                # Generate scatter plot for CMJ (Force_Peak_Power vs. Velo_Peak_Power)\n",
    "                client_cmj_dict = {'Force_at_PP': client_cmj_data[3],\n",
    "                   'Vel_at_PP':   client_cmj_data[4]}\n",
    "                reference_cmj_dict = pd.DataFrame(reference_cmj_data, columns=variables)\n",
    "                \n",
    "                # Add scatter plot title and image\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                scatter_image = generate_scatter_plot(client_cmj_dict, reference_cmj_dict,\n",
    "                                                      'Force_at_PP', 'Vel_at_PP',\n",
    "                                                      'CMJ: Force vs. Velocity', tmpdirname)\n",
    "                doc.add_picture(scatter_image, width=Inches(6))\n",
    "    \n",
    "        # ───────────────────────── DJ ─────────────────────────\n",
    "        elif movement == 'DJ':\n",
    "            # ── pull client & reference rows ──────────────────\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP, CT, RSI\n",
    "                FROM DJ\n",
    "                WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_dj_data = client_cursor.fetchone()\n",
    "    \n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP, CT, RSI\n",
    "                FROM DJ\n",
    "            \"\"\")\n",
    "            reference_dj_data = np.array(reference_cursor.fetchall())\n",
    "    \n",
    "            dj_vars = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg' ,'Force_at_PP', 'Vel_at_PP', 'CT', 'RSI']\n",
    "    \n",
    "            # ── bar graphs ────────────────────────────────────\n",
    "            if client_dj_data and reference_dj_data.size:\n",
    "                for i, var in enumerate(dj_vars):\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison\",\n",
    "                                      style='Heading 2')\n",
    "                    bar = generate_bar_graph(\n",
    "                        var,\n",
    "                        client_dj_data[i],\n",
    "                        reference_dj_data[:, i],\n",
    "                        f'{var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(bar, width=Inches(6))\n",
    "    \n",
    "            # OPTIONAL: DJ force–velocity scatter (comment out if unwanted)\n",
    "            client_dj_dict     = {'Force_at_PP': client_dj_data[3],\n",
    "                                  'Vel_at_PP'  : client_dj_data[4]}\n",
    "            reference_dj_dict  = pd.DataFrame(reference_dj_data,\n",
    "                                              columns=dj_vars)\n",
    "            doc.add_paragraph(\"Force vs. Velocity Scatter Plot\",\n",
    "                              style='Heading 2')\n",
    "            dj_scatter = generate_scatter_plot(\n",
    "                client_dj_dict, reference_dj_dict,\n",
    "                'Force_at_PP', 'Vel_at_PP',\n",
    "                'DJ: Force vs. Velocity', tmpdirname\n",
    "            )\n",
    "            doc.add_picture(dj_scatter, width=Inches(6))\n",
    "    \n",
    "        # ───────────────────────── SLV ───────────────────────\n",
    "        elif movement == 'SLV':\n",
    "            # pull left & right trials\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Left'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvl_data = client_cursor.fetchone()\n",
    "    \n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN,  PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Right'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvr_data = client_cursor.fetchone()\n",
    "    \n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP FROM SLV\n",
    "            \"\"\")\n",
    "            reference_slv_data = np.array(reference_cursor.fetchall())\n",
    "    \n",
    "            slv_vars = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg',  'Force_at_PP', 'Vel_at_PP']\n",
    "    \n",
    "            if client_slvl_data and client_slvr_data and reference_slv_data.size:\n",
    "                # ── left-vs-right histograms (JH & Force) ─────\n",
    "                for idx, var in enumerate(['JH_IN', 'Force_at_PP']):\n",
    "                    doc.add_paragraph(\n",
    "                        f\"{var.replace('_',' ')} Comparison (Left vs Right)\",\n",
    "                        style='Heading 2')\n",
    "                    hist = generate_slv_histogram(\n",
    "                        var,\n",
    "                        client_slvl_data[idx],            # left value\n",
    "                        client_slvr_data[idx],            # right value\n",
    "                        reference_slv_data[:, idx],\n",
    "                        f'{var} Comparison', tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(hist, width=Inches(6))\n",
    "    \n",
    "                # ── scatter Force vs Velocity ────────────────\n",
    "                client_slv_all = np.array([client_slvl_data, client_slvr_data])\n",
    "                client_slv_dict = {\n",
    "                    'Force_at_PP': client_slv_all[:, 3],\n",
    "                    'Vel_at_PP'  : client_slv_all[:, 4],\n",
    "                }\n",
    "                reference_slv_dict = pd.DataFrame(reference_slv_data,\n",
    "                                                  columns=slv_vars)\n",
    "    \n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\",\n",
    "                                  style='Heading 2')\n",
    "                slv_scatter = generate_scatter_plot(\n",
    "                    client_slv_dict, reference_slv_dict,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'SLV: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(slv_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"⚠️  Missing SLV data – skipping SLV graphs\")\n",
    "\n",
    "        elif movement == 'NMT':\n",
    "            # Fetch NMT data for the client (10s taps only)\n",
    "            client_cursor.execute(f\"SELECT NUM_TAPS_10s FROM NMT WHERE name = '{client_name}'\")\n",
    "            client_nmt_data = client_cursor.fetchone()\n",
    "            reference_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT\")\n",
    "            reference_nmt_data = np.array(reference_cursor.fetchall())\n",
    "            \n",
    "            # Ensure data exists before proceeding\n",
    "            if client_nmt_data and reference_nmt_data.size > 0:\n",
    "                # Format the variable name by removing underscores\n",
    "                nmt_var = 'NUM_TAPS_10s'.replace('_', ' ')\n",
    "                \n",
    "                # Add variable title before the graph\n",
    "                doc.add_paragraph(f\"{nmt_var} Comparison\", style='Heading 2')\n",
    "                \n",
    "                # Generate histogram for NMT 10s taps\n",
    "                nmt_image = generate_bar_graph('NUM_TAPS_10s', client_nmt_data[0], reference_nmt_data[:, 0], f'{nmt_var} Comparison', tmpdirname)\n",
    "                doc.add_picture(nmt_image, width=Inches(6))\n",
    "\n",
    "# Function to convert DOCX to images\n",
    "def docx_to_images(docx_path, output_dir):\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(docx_path)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Create a blank image with white background\n",
    "    img_width, img_height = 1000, 1500\n",
    "    image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Use a simple font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw the text onto the image\n",
    "    padding = 20\n",
    "    y_text = padding\n",
    "    for line in lines:\n",
    "        if y_text + padding > img_height:\n",
    "            # Save the image and start a new one if the text exceeds the page height\n",
    "            img_path = os.path.join(output_dir, f\"page_{int(y_text / img_height)}.png\")\n",
    "            image.save(img_path)\n",
    "            y_text = padding\n",
    "            image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # Calculate text size and draw it\n",
    "        text_bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        draw.text((padding, y_text), line, font=font, fill=\"black\")\n",
    "        y_text += text_height + padding\n",
    "\n",
    "    # Save the last image\n",
    "    img_path = os.path.join(output_dir, \"final_page.png\")\n",
    "    image.save(img_path)\n",
    "\n",
    "    return img_path\n",
    "\n",
    "doc.save(output_filename)          # ← only one final save\n",
    "print(f\"Document saved at: {output_filename}\")\n",
    "\n",
    "# Close connections\n",
    "client_conn.close()\n",
    "reference_conn.close()\n",
    "\n",
    "# Example usage\n",
    "img_output_directory = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\Pro Reports\\Images'\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# Convert DOCX to images\n",
    "img_path = docx_to_images(output_filename, img_output_directory)\n",
    "print(f\"Images saved at {img_path}\")"
   ],
   "id": "651bafa1b33962ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 3 rows to CMJ in Athletic_Screen_Pro_data_v2.db\n",
      "Copied 3 rows to CMJ in all\n",
      "Copied 3 rows to DJ in Athletic_Screen_Pro_data_v2.db\n",
      "Copied 3 rows to DJ in all\n",
      "Copied 6 rows to SLV in Athletic_Screen_Pro_data_v2.db\n",
      "Copied 6 rows to SLV in all\n",
      "Copied 1 rows to NMT in Athletic_Screen_Pro_data_v2.db\n",
      "Copied 1 rows to NMT in all\n",
      "Data successfully copied to each target and combined database.\n"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Paths to the source and target databases\n",
    "source_db_path = 'D:/Athletic Screen 2.0/Output Files/movement_database_v2.db'\n",
    "output_folder = 'D:/Athletic Screen 2.0/Output Files/'\n",
    "target_databases = ['Athletic_Screen_Pro_data_v2.db']\n",
    "all_data_db_path = os.path.join(output_folder, 'Athletic_Screen_All_data_v2.db')\n",
    "\n",
    "# Retry mechanism for handling the locked database error\n",
    "def retry_execute(func):\n",
    "    retries = 5  # Number of retries\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            func()\n",
    "            break\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if 'database is locked' in str(e):\n",
    "                print(\"Database is locked, retrying...\")\n",
    "                time.sleep(1)  # Wait for 1 second before retrying\n",
    "                retries -= 1\n",
    "            else:\n",
    "                raise e\n",
    "        if retries == 0:\n",
    "            raise Exception(\"Max retries reached. Database is still locked.\")\n",
    "\n",
    "# Table schemas to create in the target databases and the combined database\n",
    "table_schemas = {\n",
    "    'CMJ': '''CREATE TABLE IF NOT EXISTS CMJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "\n",
    "    'DJ':  '''CREATE TABLE IF NOT EXISTS DJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL,\n",
    "                CT REAL,\n",
    "                RSI REAL\n",
    "              )''',\n",
    "\n",
    "    'SLV': '''CREATE TABLE IF NOT EXISTS SLV (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT, \n",
    "                trial_name TEXT,\n",
    "                side TEXT,\n",
    "                JH_IN REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "    'NMT': '''CREATE TABLE IF NOT EXISTS NMT (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                NUM_TAPS_10s REAL,\n",
    "                NUM_TAPS_20s REAL,\n",
    "                NUM_TAPS_30s REAL,\n",
    "                NUM_TAPS REAL\n",
    "              )'''\n",
    "}\n",
    "# Function to create tables in a database connection\n",
    "def create_tables(conn):\n",
    "    cursor = conn.cursor()\n",
    "    for schema in table_schemas.values():\n",
    "        cursor.execute(schema)\n",
    "    conn.commit()\n",
    "\n",
    "# Open the source database\n",
    "source_conn = sqlite3.connect(source_db_path, timeout=10)\n",
    "source_cursor = source_conn.cursor()\n",
    "\n",
    "# Create connections to all target databases and combined database\n",
    "target_conns = {db_name: sqlite3.connect(os.path.join(output_folder, db_name), timeout=10) for db_name in target_databases}\n",
    "target_conns['all'] = sqlite3.connect(all_data_db_path, timeout=10)\n",
    "\n",
    "# Ensure tables exist in each database\n",
    "for conn in target_conns.values():\n",
    "    create_tables(conn)\n",
    "\n",
    "# Function to copy data from one table in the source to target databases\n",
    "def copy_table_data(table_name):\n",
    "    # Fetch all data except the 'id' column from the source table\n",
    "    source_cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    rows = source_cursor.fetchall()\n",
    "\n",
    "    # For each row, insert it into each target database\n",
    "    for db_name, conn in target_conns.items():\n",
    "        cursor = conn.cursor()\n",
    "        placeholders = \", \".join([\"?\"] * len(rows[0][1:]))  # Skips the first 'id' column\n",
    "        query = f\"INSERT INTO {table_name} ({', '.join([desc[0] for desc in source_cursor.description][1:])}) VALUES ({placeholders})\"\n",
    "        retry_execute(lambda: cursor.executemany(query, [row[1:] for row in rows]))  # Exclude 'id' column for insertion\n",
    "        conn.commit()\n",
    "        print(f\"Copied {len(rows)} rows to {table_name} in {db_name}\")\n",
    "\n",
    "# List of table names to copy data\n",
    "tables_to_copy = ['CMJ', 'DJ', 'SLV', 'NMT']\n",
    "\n",
    "# Copy data from each table\n",
    "for table in tables_to_copy:\n",
    "    copy_table_data(table)\n",
    "\n",
    "# Close all connections\n",
    "source_conn.close()\n",
    "for conn in target_conns.values():\n",
    "    conn.close()\n",
    "\n",
    "print(\"Data successfully copied to each target and combined database.\")\n",
    "\n",
    "# Path to the folder containing ASCII .txt files\n",
    "ascii_folder = r\"D:/Athletic Screen 2.0/Output Files/\"\n",
    "\n",
    "# Remove all .txt files in the ascii_folder\n",
    "for filename in os.listdir(ascii_folder):\n",
    "    if filename.lower().endswith(\".txt\"):\n",
    "        file_path = os.path.join(ascii_folder, filename)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "print(\"All ASCII .txt files cleared after ingestion.\")\n"
   ],
   "id": "acbe8b04e0392085"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:41:45.910825Z",
     "start_time": "2025-08-13T19:41:45.848520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "# Paths\n",
    "db_source_paths = [\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/movement_database_v2.db\",\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/Athletic_Screen_All_data_v2.db\",\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/Athletic_Screen_Pro_data_v2.db\"\n",
    "]\n",
    "destination_folder = r\"G:/My Drive/Data/Athletic Screen Data\"\n",
    "\n",
    "# Copy each DB file to the destination\n",
    "for db_path in db_source_paths:\n",
    "    try:\n",
    "        shutil.copy(db_path, destination_folder)\n",
    "        print(f\"Copied {db_path} → {destination_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {db_path}: {e}\")\n"
   ],
   "id": "714e986e51635beb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied D:/Athletic Screen 2.0/Output Files/movement_database_v2.db → G:/My Drive/Data/Athletic Screen Data\n",
      "Copied D:/Athletic Screen 2.0/Output Files/Athletic_Screen_All_data_v2.db → G:/My Drive/Data/Athletic Screen Data\n",
      "Copied D:/Athletic Screen 2.0/Output Files/Athletic_Screen_Pro_data_v2.db → G:/My Drive/Data/Athletic Screen Data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89aff8f2659e8154"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
